
<!-- saved from url=(0020)http://www.zptu.net/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

	
<title>Yan Wang's Homepage</title>

<style type="text/css">
    body {
        font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
        font-size: 18px;
        margin: 0 auto;
        width: 90%;
        min-width: 200px;
        max-width: 1200px;
        width: expression_r(Math.max(200, Math.min(1200, document.body.offsetWidth-40)) + "px");
    }
</style>

<style>
table {
    font-family: cambria, calibri, garamond, century, gulim, dotum, arial;
    border-collapse: collapse;
    width: 100%;
    font-size:18px;}
</style>

</head>
<body>


<!-- img border="0" height="276.3" width="337.5" src="images/zptu.jpg">

<h2><b>Zhaopeng Tu</b></h2>
<p align="left">
<b>Principal Researcher</b>
<br/>
<a href="http://ai.tencent.com/ailab/">Tencent AI Lab</a><br />
tuzhaopeng@gmail.com
</p -->

<table class="wsite-multicol-table">
<tbody class="wsite-multicol-tbody">
<tr class="wsite-multicol-tr">

<td class="wsite-multicol-col">
    <img src="./Zhaopeng Tu&#39;s Homepage_files/head.jpg" height="204.75" width="307.2" border="0" align="center">

    <br>
    <h2>Yan Wang</h2>

    <p>Senior Researcher<br>
    <a href="http://ai.tencent.com/ailab/">Tencent AI Lab</a><br>
    yanwang.branden@gmail.com</p>
</td>

</tr>
</tbody>
</table>


<h2>Bio</h2>
<p>Yan Wang is a senior researcher at Tencent AI Lab, whose research focuses on dialogue generation and text generation.
He is currently working on retrieval-guided dialogue generation, multi-turn dialogue generation and presona-consistency in dialogue.
In last three years he has published more than ten papers in leading NLP/AI conferences such as ACL, EMNLP and AAAI. He also develop
a <a href="https://ai.qq.com/product/nlpchat.shtml">commercial conversational system</a> in Tencent Cloud that provides chitchat services to partners with more than 10 million requests every day.
</p>

<p></p>
<center><font color="#008F00">Please refer to <a href="./intern.html">research intern positions</a> for the information about internship.</font></center>

<h2>News</h2>

<p></p>


<ul>
  <li type="circle">09/14/2020: Four papers were accepted to COLING2020.</li>
  <li type="circle">09/14/2020: Three papers were accepted to EMNLP2020.</li>
  <li type="circle">04/04/2020: Two papers were accepted to ACL2020.</li>
  <li type="circle">11/11/2019: Three papers were accepted to AAAI2020.</li>
  <li type="circle">08/13/2019: Seven papers were accepted to EMNLP2019.</li>
  <li type="circle">05/15/2019: Two papers were accepted to ACL2019.</li>
  <li type="circle">02/23/2019: Four papers were accepted to NAACL2019.</li>
  <li type="circle">11/01/2018: Three papers were accepted to AAAI2019.</li>
  <!-- li type="circle">08/10/2018: Four papers were accepted to EMNLP2018.</li-->
</ul>

<p></p>


<h2>Selected Talks</h2>

<ul>
  <li type="circle">07/29/2019: A talk on Tencent Academic and Industrial Conference (TAIC) in ACL2019, which summarizes our recent work in machine translation. [<a href="http://www.zptu.net/talks/20190729%20Information%20Transformation.pdf"><font color="#0080FF">slides</font></a>]</li>
  <li type="circle">07/27/2018: A lecture in CIPS Summer School, which gives a survey of recent progresses on improving NMT models (covering 70 papers). [<a href="http://www.zptu.net/talks/20180727_CIPS_Summer_School.pdf"><font color="#0080FF">slides (in Chinese)</font></a>]
  </li><li type="circle">01/04/2018: A talk on <i>Adequacy-Oriented NMT</i>, which summarizes our recent papers that were accepted in the last year. [<a href="http://www.zptu.net/talks/adequacy-oriented_nmt.pdf"><font color="#0080FF">slides</font></a>] [<a href="https://mp.weixin.qq.com/s/joZnfPtR3vv6LJPsRrY9Jw"><font color="#0080FF">video and textual explication (in Chinese)</font></a>]</li>
</ul>

<p></p>


<h2>Selected Publications</h2> 

<a href="./publications.html">full list</a>

<p></p>

<h3>Adequate NMT</h3>
<ul>
    <li type="circle"><u>Zhaopeng Tu</u>, Zhengdong Lu, Yang Liu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1601.04811"><font color="#0080FF">Modeling Coverage for Neural Machine Translation</font></a>. <i>ACL 2016</i>. [<a href="https://github.com/tuzhaopeng/NMT-Coverage">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Zhengdong Lu, Xiaohua Liu, and Hang Li. <a href="http://arxiv.org/abs/1608.06043"><font color="#0080FF">Context Gates for Neural Machine Translation</font></a>. <i>TACL 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Lifeng Shang, Xiaohua Liu, and Hang Li. <a href="https://arxiv.org/abs/1611.01874"><font color="#0080FF">Neural Machine Translation with Reconstruction</font></a>. <i>AAAI 2017</i>. [<a href="https://github.com/tuzhaopeng/NMT">code</a>]</li>
    <li type="circle">Yong Cheng, <u>Zhaopeng Tu</u>, Fandong Meng, Junjie Zhai, and Yang Liu. <a href="https://arxiv.org/abs/1805.06130"><font color="#0080FF">Towards Robust Neural Machine Translation</font></a>. <i>ACL 2018</i>. </li>
    <li type="circle">Xiang Kong, <u>Zhaopeng Tu</u>*, Shuming Shi, Eduard Hovy, and Tong Zhang. <a href="https://arxiv.org/abs/1811.08541"><font color="#0080FF">Neural Machine Translation with Adequacy-Oriented Learning</font></a>. <i>AAAI 2019</i>. </li>

</ul>

<h3>Model Interpretation</h3>
<ul>
    <li type="circle">Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1906.00592"><font color="#0080FF">Assessing the Ability of Self-Attention Networks to Learn Word Order</font></a>. <i>ACL 2019</i>. </li>
    <li type="circle">Shilin He, <u>Zhaopeng Tu</u>*, Xing Wang, Longyue Wang, Michael R. Lyu, and Shuming Shi. <a href="https://arxiv.org/abs/1909.00326"><font color="#0080FF">Towards Understanding Neural Machine Translation with Word Importance</font></a>. <i>EMNLP 2019</i>. </li>
    <li type="circle">Shuo Wang, <u>Zhaopeng Tu</u>, Shuming Shi, and Yang Liu. <font color="#0080FF">On the Inference Calibration of Neural Machine Translation</font>. <i>ACL 2020</i>. </li>
    <li type="circle">Xinwei Geng, Longyue Wang, Xing Wang, Bing Qin, Ting Liu, and <u>Zhaopeng Tu</u>. <font color="#0080FF">How Does Selective Mechanism Improve Self-Attention Networks?</font> <i>ACL 2020</i>. </li>
    <li type="circle">Yilin Yang, Longyue Wang, Shuming Shi, Prasad Tadepalli, Stefan Lee, and <u>Zhaopeng Tu</u>. <font color="#0080FF">On the Sub-Layer Functionalities of Transformer Decoder</font>. <i>EMNLP 2020 (Findings)</i>.</li>
    <li type="circle">Yong Wang, Longyue Wang, Victor O.K. Li, and <u>Zhaopeng Tu</u>. <font color="#0080FF">On the Sparsity of Neural Machine Translation Models</font>. <i>EMNLP 2020 (Short)</i>.</li>
    <li type="circle">Wenxuan Wang and <u>Zhaopeng Tu</u>*. <font color="#0080FF">Rethinking the Value of Transformer Components</font>. <i>COLING 2020</i>.</li>

</ul>

<h3>Self-Attention Network</h3>
<ul>
    <li type="circle"><i>Context Modeling</i></li>
    <ul>
        <li type="disc">Baosong Yang, <u>Zhaopeng Tu</u>*, Derek F. Wong, Fandong Meng, Lidia S. Chao, and Tong Zhang. <a href="https://arxiv.org/abs/1810.10182"><font color="#0080FF">Modeling Localness for Self-Attention Networks</font></a>. <i>EMNLP 2018</i>. </li>
        <li type="disc">Baosong Yang, Jian Li, Derek F. Wong, Lidia S. Chao, Xing Wang, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1902.05766"><font color="#0080FF">Context-Aware Self-Attention Networks</font></a>. <i>AAAI 2019</i>. </li>
        <li type="disc">Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S. Chao, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1904.03107"><font color="#0080FF">Convolutional Self-Attention Networks</font></a>. <i>NAACL 2019 (Short)</i>. </li>
    </ul>
    
    <li type="circle"><i>Multi-Head Attention</i></li>
    <ul>
        <li type="disc">Jian Li, <u>Zhaopeng Tu</u>*, Baosong Yang, Michael R. Lyu, and Tong Zhang. <a href="https://arxiv.org/abs/1810.10183"><font color="#0080FF">Multi-Head Attention with Disagreement Regularization</font></a>. <i>EMNLP 2018 (Short)</i>.</li>
        <li type="disc">Jian Li, Baosong Yang, Zi-Yi Dou, Xing Wang, Michael R. Lyu, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1904.03100"><font color="#0080FF">Information Aggregation for Multi-Head Attention with Routing-by-Agreement</font></a>. <i>NAACL 2019</i>. </li>
        <li type="disc">Jian Li, Xing Wang, Baosong Yang, Shuming Shi, Michael R. Lyu, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1911.09877"><font color="#0080FF">Neuron Interaction Based Representation Composition for Neural Machine Translation</font></a>. <i>AAAI 2020</i>. </li>

    </ul>
    
    <li type="circle"><i>Structure Modeling</i></li>
    <ul>
        <li type="disc">Jie Hao, Xing Wang, Baosong Yang, Longyue Wang, Jinfeng Zhang, and <u>Zhaopeng Tu</u>*. <a href="https://arxiv.org/abs/1904.03092"><font color="#0080FF">Modeling Recurrence for Transformer</font></a>. <i>NAACL 2019</i>. </li>
        <li type="disc">Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/1909.02222"><font color="#0080FF">Multi-Granularity Self-Attention for Neural Machine Translation</font></a>. <i>EMNLP 2019</i>.</li>
        <li type="disc">Jie Hao, Xing Wang, Shuming Shi, Jinfeng Zhang, and <u>Zhaopeng Tu</u>. <a href="https://arxiv.org/abs/1909.01562"><font color="#0080FF">Towards Better Modeling Hierarchical Structure for Self-Attention with Ordered Neurons</font></a>. <i>EMNLP 2019 (Short)</i>.</li>
        <li type="disc">Xing Wang, <u>Zhaopeng Tu</u>, Longyue Wang, and Shuming Shi. <a href="https://arxiv.org/abs/1909.00383"><font color="#0080FF">Self-Attention Networks with Structural Position Encoding</font></a>. <i>EMNLP 2019 (Short)</i>. </li>
    </ul>
    
    <li type="circle"><i>Deep Representations</i></li>
    <ul>
        <li type="disc">Zi-Yi Dou, <u>Zhaopeng Tu</u>*, Xing Wang, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1810.10181"><font color="#0080FF">Exploiting Deep Representations for Neural Machine Translation</font></a>. <i>EMNLP 2018</i>. </li>
        <li type="disc">Zi-Yi Dou, <u>Zhaopeng Tu</u>*, Xing Wang, Longyue Wang, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1902.05770"><font color="#0080FF">Dynamic Layer Aggregation for Neural Machine Translation with Routing-by-Agreement</font></a>. <i>AAAI 2019</i>. </li>
        <li type="disc">Xing Wang, <u>Zhaopeng Tu</u>, Longyue Wang, and Shuming Shi. <a href="https://arxiv.org/abs/1906.01268"><font color="#0080FF">Exploiting Sentential Context for Neural Machine Translation</font></a>. <i>ACL 2019 (Short)</i>. </li>
    </ul>
</ul>

<h3>Document NMT</h3>
<ul>    
    <li type="circle">Longyue Wang, <u>Zhaopeng Tu</u>*, Andy Way, and Qun Liu. <a href="https://arxiv.org/abs/1704.04347"><font color="#0080FF">Exploiting Cross-Sentence Context for Neural Machine Translation</font></a>. <i>EMNLP 2017 (Short)</i>. </li>
    <li type="circle"><u>Zhaopeng Tu</u>, Yang Liu, Shuming Shi, and Tong Zhang. <a href="https://arxiv.org/abs/1711.09367"><font color="#0080FF">Learning to Remember Translation History with a Continuous Cache</font></a>. <i>TACL 2018</i>. </li>
</ul>

<p></p>





<p></p>

<h2>Selected Professional Services</h2>

<p></p>

<h3>Jounrals</h3>
<ul>
    <li type="circle">NeuroComputing: Associate Editor (2020-)</li>
    <li type="circle">Computational Linguistics: Reviewer (2016-)</li>
</ul>

<h3>Conferences</h3>
<ul>
    <li type="circle">ACL: Area Chair (2019), Program Committee (2017,2018,2020)</li>
    <li type="circle">EMNLP: Area Chair (2018,2019), Program Committee (2020)</li>
    <li type="circle">NAACL: Area Chair (2019)</li>
    <li type="circle">AAAI: Senior Program Committee (2019)</li>
    <li type="circle">IJCAI: Senior Program Committee (2021)</li>
</ul>


</body><div style="all: initial;"><div></div></div></html>